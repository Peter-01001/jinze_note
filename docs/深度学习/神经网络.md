# 神经网络

神经网络是一种计算模型，它将输入数据映射到输出数据。

## 一些思想

+ **end to end（端到端）**：中间内容交给神经网路，不看中间过程

## 神经网络架构

+ **输入层（Input Layer）**：接收原始数据，相当于 “神经网络的眼睛”

+ **隐藏层（Hidden Layer）**：提取数据特征，相当于 “神经网络的大脑”

+ **输出层（Output Layer）**：输出最终结果，相当于 “神经网络的嘴巴”

### 线性函数

+ **线性函数**：$y = wx + b$

+ **$w$**：权重

+ **$b$**：偏置

其中权重和偏置随机初始化或使用预训练的参数，并在训练过程中不断调整

### softmax分类器

线性函数的输出结果是一个向量，softmax分类器将向量映射到概率分布，即每个类别的概率

+ **softmax激活函数**：$y_i = \frac{e^{w_i x + b_i}}{\sum_{j=1}^n e^{w_j x + b_j}}$

+ **sigmoid激活函数**：$y = \frac{1}{1 + e^{-x}}$（不属于softmax分类器）

其中softmax用于多分类问题，sigmoid用于二分类问题

+ **交叉熵损失函数**：$Loss = -\sum_{i=1}^n y_i log(y_i)$

概率为1时没有损失，概率越小损失越大

+ **流程**：放大差异（e的x次幂）-->归一化-->计算损失

### 前向传播

输入数据经过神经网络，得到输出数据，输出数据作为下一层的输入数据，重复此过程，直到输出层

+ **batch**：批处理数量，如32

**e.g.**:32×64×64×3 一次将32个样本拼接

### 反向传播

由损失更新权重和偏置

+ **梯度下降**：$w = w - \alpha \frac{\partial Loss}{\partial w}$
  
向损失小的方向更新

+ **动量**：$w = w - \alpha \frac{\partial Loss}{\partial w} + \beta \frac{\partial Loss}{\partial w}$

每次迭代独立，但使用动量可加快收敛速度

+ **学习率（步长）**：越小越好，一般先大（warmup）后小

w1、w2……wn更新之间无关系

### 数据预处理

### dropout

随机丢弃一些神经元，防止过拟合

+ 一般在全连接层加上，输出层不加
